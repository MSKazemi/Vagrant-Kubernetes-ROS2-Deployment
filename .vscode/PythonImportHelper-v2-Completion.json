[
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "gzip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gzip",
        "description": "gzip",
        "detail": "gzip",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "PrometheusConnect",
        "importPath": "prometheus_api_client",
        "description": "prometheus_api_client",
        "isExtraImport": true,
        "detail": "prometheus_api_client",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "json_normalize",
        "importPath": "pandas",
        "description": "pandas",
        "isExtraImport": true,
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "display",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "prom",
        "kind": 5,
        "importPath": "DataAnalysis.backup",
        "description": "DataAnalysis.backup",
        "peekOfCode": "prom = PrometheusConnect(url =\"http://192.168.56.10:30090\", disable_ssl=True)\n# Fetch all metric names\nall_metrics = prom.all_metrics()\n# Define a starting point for data collection\nstart_time = datetime.now() - timedelta(days=1)  # Example: 5 years ago\nend_time = datetime.now()  # Current time\nstep = '20s'  # Step size\n# if there is no path and dir is not exist will create it recursively\npath = f\"../data_json/{datetime.now().strftime('%Y%m%d')}/\"\nos.makedirs(path, exist_ok=True)",
        "detail": "DataAnalysis.backup",
        "documentation": {}
    },
    {
        "label": "all_metrics",
        "kind": 5,
        "importPath": "DataAnalysis.backup",
        "description": "DataAnalysis.backup",
        "peekOfCode": "all_metrics = prom.all_metrics()\n# Define a starting point for data collection\nstart_time = datetime.now() - timedelta(days=1)  # Example: 5 years ago\nend_time = datetime.now()  # Current time\nstep = '20s'  # Step size\n# if there is no path and dir is not exist will create it recursively\npath = f\"../data_json/{datetime.now().strftime('%Y%m%d')}/\"\nos.makedirs(path, exist_ok=True)\nfor metric in all_metrics:\n    # Perform a range query for each metric",
        "detail": "DataAnalysis.backup",
        "documentation": {}
    },
    {
        "label": "start_time",
        "kind": 5,
        "importPath": "DataAnalysis.backup",
        "description": "DataAnalysis.backup",
        "peekOfCode": "start_time = datetime.now() - timedelta(days=1)  # Example: 5 years ago\nend_time = datetime.now()  # Current time\nstep = '20s'  # Step size\n# if there is no path and dir is not exist will create it recursively\npath = f\"../data_json/{datetime.now().strftime('%Y%m%d')}/\"\nos.makedirs(path, exist_ok=True)\nfor metric in all_metrics:\n    # Perform a range query for each metric\n    result_range = prom.custom_query_range(\n        query=metric,",
        "detail": "DataAnalysis.backup",
        "documentation": {}
    },
    {
        "label": "end_time",
        "kind": 5,
        "importPath": "DataAnalysis.backup",
        "description": "DataAnalysis.backup",
        "peekOfCode": "end_time = datetime.now()  # Current time\nstep = '20s'  # Step size\n# if there is no path and dir is not exist will create it recursively\npath = f\"../data_json/{datetime.now().strftime('%Y%m%d')}/\"\nos.makedirs(path, exist_ok=True)\nfor metric in all_metrics:\n    # Perform a range query for each metric\n    result_range = prom.custom_query_range(\n        query=metric,\n        start_time=start_time,",
        "detail": "DataAnalysis.backup",
        "documentation": {}
    },
    {
        "label": "step",
        "kind": 5,
        "importPath": "DataAnalysis.backup",
        "description": "DataAnalysis.backup",
        "peekOfCode": "step = '20s'  # Step size\n# if there is no path and dir is not exist will create it recursively\npath = f\"../data_json/{datetime.now().strftime('%Y%m%d')}/\"\nos.makedirs(path, exist_ok=True)\nfor metric in all_metrics:\n    # Perform a range query for each metric\n    result_range = prom.custom_query_range(\n        query=metric,\n        start_time=start_time,\n        end_time=end_time,",
        "detail": "DataAnalysis.backup",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "DataAnalysis.backup",
        "description": "DataAnalysis.backup",
        "peekOfCode": "path = f\"../data_json/{datetime.now().strftime('%Y%m%d')}/\"\nos.makedirs(path, exist_ok=True)\nfor metric in all_metrics:\n    # Perform a range query for each metric\n    result_range = prom.custom_query_range(\n        query=metric,\n        start_time=start_time,\n        end_time=end_time,\n        step=step\n    )",
        "detail": "DataAnalysis.backup",
        "documentation": {}
    },
    {
        "label": "read_json",
        "kind": 2,
        "importPath": "DataAnalysis.helper",
        "description": "DataAnalysis.helper",
        "peekOfCode": "def read_json(file_path):\n    with open(file_path, 'rt', encoding='utf-8') as file:\n        data = json.load(file)\n    print(f\"File: {file}\\n\")\n    print(\"--- keys --------------------\" + 20*'--')\n    print(data[0].keys())\n    print(\"--- Metric Keys -------------\" + 20*'--')\n    print(data[0]['metric'].keys())\n    df = json_normalize(data)\n    print(\"\\nUnique values in each column except values columns: ------------------\")",
        "detail": "DataAnalysis.helper",
        "documentation": {}
    },
    {
        "label": "filter_filenames",
        "kind": 2,
        "importPath": "DataAnalysis.helper",
        "description": "DataAnalysis.helper",
        "peekOfCode": "def filter_filenames(path, substrings, and_or):\n    # Normalize substrings to lower case for case-insensitive comparison\n    substrings = [s.lower() for s in substrings]\n    # Initialize list to hold matching filenames\n    matching_files = []\n    # Walk through the directory\n    try:\n        # List all files in the directory\n        files = os.listdir(path)\n    except Exception as e:",
        "detail": "DataAnalysis.helper",
        "documentation": {}
    },
    {
        "label": "get_filename_and_root_path",
        "kind": 2,
        "importPath": "DataAnalysis.helper",
        "description": "DataAnalysis.helper",
        "peekOfCode": "def get_filename_and_root_path(file_path):\n    # Extract the file name from the path\n    file_name = os.path.basename(file_path)\n    # Extract the directory (root path) from the path\n    root_path = os.path.dirname(file_path)\n    return file_name, root_path\ndef json_2_pandas(json_file_path):\n    with open(json_file_path, 'rt', encoding='utf-8') as file:\n        data = json.load(file)\n    df = json_normalize(data)",
        "detail": "DataAnalysis.helper",
        "documentation": {}
    },
    {
        "label": "json_2_pandas",
        "kind": 2,
        "importPath": "DataAnalysis.helper",
        "description": "DataAnalysis.helper",
        "peekOfCode": "def json_2_pandas(json_file_path):\n    with open(json_file_path, 'rt', encoding='utf-8') as file:\n        data = json.load(file)\n    df = json_normalize(data)\n    file_name, root_path = get_filename_and_root_path(json_file_path)\n    csv_file_path = os.path.join(root_path+\"/csv/\",file_name.replace('.json', '.csv'))\n    if not os.path.exists(os.path.dirname(root_path+\"/csv/\")):\n        os.makedirs(os.path.dirname(root_path+\"/csv/\"))\n    df.to_csv(csv_file_path, index=False)\n    return df",
        "detail": "DataAnalysis.helper",
        "documentation": {}
    },
    {
        "label": "transform_data",
        "kind": 2,
        "importPath": "DataAnalysis.helper",
        "description": "DataAnalysis.helper",
        "peekOfCode": "def transform_data(data):\n    # Convert the string representation of list in the 'values' column to actual lists\n    # data['values'] = data['values'].apply(ast.literal_eval)\n    display(data)\n    # Explode the 'values' column into multiple rows\n    exploded_data = data.explode('values')\n    # Split the 'values' column into 'timestamp' and 'value' columns\n    exploded_data[['timestamp', 'value']] = pd.DataFrame(exploded_data['values'].tolist(), index=exploded_data.index)\n    # Now let's display the first few rows of the transformed DataFrame\n    display(exploded_data)",
        "detail": "DataAnalysis.helper",
        "documentation": {}
    },
    {
        "label": "unique_values",
        "kind": 2,
        "importPath": "DataAnalysis.helper",
        "description": "DataAnalysis.helper",
        "peekOfCode": "def unique_values(data):\n    unique_values = {col: data[col].unique() for col in data.columns}\n    for column, values in unique_values.items():\n        print(f\"Unique values for {column}: {values}\")\n    return unique_values",
        "detail": "DataAnalysis.helper",
        "documentation": {}
    }
]